Ok, voici une idée pour un nouveau produit qui est un peu comme une question à un expert. Donc, en fait, n'importe quelle personne dans l'entreprise pourrait, quand il y a une question sur peu importe le sujet, il pourrait poser la question dans cet outil-là. Donc, que ce soit un genre de chatbot, tu peux poser ta question et ça te donne la réponse, si la réponse existe dans le knowledge base de l'entreprise en quelque sorte. Et l'idée de cet outil-là, c'est que le knowledge base pourrait se créer au fur et à mesure. Donc, ça c'est l'idée principale là-dedans. C'est-à-dire, si la réponse à la question existe dans l'information, l'agent va aller faire un RAG dans la documentation pour voir, cette question-là qui est posée, on a-t-il une réponse à ça? Si il y en a une réponse, il va donner la réponse directement à la personne en donnant la source, ou les réponses en donnant les sources. Et si il n'a pas la réponse à la question, il va notifier le bon expert dans l'entreprise qui pourrait potentiellement répondre à cette question-là. La question va être posée à un expert qui va recevoir une notification, par exemple un courriel, on va commencer, mais aussi une notification SMS qui va lui dire, il y a une question qui demande ton expertise, avec le lien direct, pour pouvoir aller sur l'application web et répondre à la question. Puis là, il faut que ça soit très simple pour lui, c'est-à-dire qu'il se connecte avec son téléphone, la version mobile, la version web-based est très responsive, elle fonctionne très bien. Quand il se connecte sur le lien, ça va directement à la bonne question, puis il peut y répondre directement, soit dans le champ de texte, ou en démarrant le micro. Quand il démarre le micro, il parle, quand il arrête le micro, ça transcrit, puis il peut soumettre la réponse. Puis là, il peut penser à d'autres questions qui mériteraient son expertise. Ces réponses-là vont aller se sauvegarder dans la base de données et dans le RAG, donc ça va aller se mettre dans la base vectorielle. Je n'ai pas besoin que ça développe toute la base vectorielle, le embed et tout ça dans le logiciel, ça on va le développer en parallèle sur n8n, mais ça va envoyer la bonne réponse, ça va appeler une route qui va aller mettre l'information dans le RAG, ça va appeler une route RAG qu'on va créer, pour aller populer la base de données vectorielle. Et même chose dans l'autre sens, quand le chatbot va donner les réponses, lui, il va aller piger dans le RAG, même chose on va le faire dans n8n, où il va avoir accès à la base vectorielle. Il va juste faire des requêtes à un agent qui a accès à cette base vectorielle-là. Il va faire un webbook, ça va être un endpoint, il va faire un GET ou un POST, puis il va attendre la réponse. La réponse qu'il va avoir pour l'utilisateur, la réponse à la question, mais aussi la ressource qui est un peu la question-réponse initiale. Je ne sais pas exactement comment on pourrait le faire pour que ça soit clair, mais il faudrait que dans le chatbot, ça réponde la réponse, avec son niveau de certitude sur le fait que c'est un lien ou non, puis de donner les sources, les accès aux sources facilement. Il peut cliquer sur la source puis l'avoir dans un pop-up qui montre c'était quoi la question-réponse, en quelque sorte qui a permis de donner au chatbot cette information-là. Ce qui serait intéressant, c'est si l'utilisateur, comme je mentionnais, si l'utilisateur pose une question qui n'est pas connue par le knowledge base, on ne veut pas que l'agent lui invente l'information. Par contre, on veut vraiment qu'il notifie la bonne personne. Il va falloir que dans le CHIEL, on puisse créer des experts qui leur disent c'est quoi leur rôle, c'est quoi le genre de question qu'ils peuvent répondre, sur quel sujet, quel aspect, puis tout ça, on va le donner en natural language. On va créer un expert, on va mettre son titre, son adresse courriel, son numéro de téléphone, pour pouvoir le notifier, mais on va aussi mettre un texte qui décrit vraiment toute son expertise. L'utilisateur va pouvoir spécifier toute son expertise, soit par écrit ou avec le micro de transcription. Quand il va avoir une question qui n'a pas pu être réponde par l'agent, il va aller regarder dans sa liste d'experts, quel expert, puis c'est peut-être au pluriel, serait le plus propice à pouvoir répondre à ces questions-là. Lesquels des experts pourraient répondre, ou lequel. Il va notifier cette personne-là, qui va recevoir un email et un message texte. Si dans les paramètres d'utilisateur, la notification par email et par message texte. Il faut que ça soit configurable. Lui, quand il va se connecter, il va recevoir un lien direct. Quand il va se connecter, ça va se connecter à la plateforme des questions-réponses, les questions qui attendent une réponse de son expertise. Il va pouvoir faire deux choses. Soit il répond, il la soumette, et ensuite passer aux autres questions qui pourraient ne pas être répondues. Il verrait très vite à qui ils ont été associés. À ces questions-là, à quel expert ça a été associé. L'option, c'est soit d'y répondre. Le prochain expert n'aura pas besoin de répondre. Quand il va se connecter, ça va dire que la question a déjà été répondue par un autre expert. Il va pouvoir juste compléter s'il veut ou pas. Ça, c'est la première option. Soit il répond, soit il la redirige à quelqu'un d'autre. C'est-à-dire que ce n'est pas lui qui a l'information. Puis, de pouvoir la rediriger à un autre expert dans la liste des experts. Ça serait cool que le UI soit vraiment simple, moderne. Quand on redirige à un expert, tu cliques sur un bouton, tu vois la liste des experts avec leurs photos, leurs noms, leurs titres. C'est vraiment une belle liste que tu peux scroller si tu es sur mobile facilement. Tu peux cliquer sur l'expert en question et lui associer. Tout le monde a le potentiel d'être un expert. Tous les utilisateurs peuvent poser des questions au chatbot. Ils peuvent voir l'historique de leur conversation facilement. Ils peuvent aussi répondre à des questions si c'est leur expertise. Ce qui serait cool, ce serait de bien classer tout l'ensemble du knowledge. D'avoir une possibilité pour que le RAC se classe d'une façon logique pour l'entreprise. Peut-être que dans certaines entreprises, il y aurait des clients. Il faudrait que la question soit associée à un client spécifique. Je ne sais pas comment on pourrait le faire exactement, mais que les questions-réponses soient un peu catégorisées. Peut-être que la même question pour deux clients différents, ce n'est pas la même réponse. Je dis les clients, mais une autre entreprise, ça va peut-être être des départements, je ne sais pas quoi, toutes sortes d'affaires. Il faut réfléchir à comment notre RAC, on peut le configurer pour qu'il vienne bien catégorisé. Quand l'utilisateur pose des questions, ça sache dans quelle catégorie aller visiter. Je n'ai pas exactement les idées pour ça, mais ça serait intéressant. L'autre chose serait les mises à jour. Il faut faire attention pour ne pas que des réponses qui ont été données il y a trois ans, qui ne sont peut-être plus à jour, les gens se servent de ça. Il faudrait faire un principe que si un expert avait fourni une réponse à une question, Il faut qu'il soit notifié si la réponse qu'il avait donnée, l'expertise qu'il avait fournie, a servi à quelqu'un qui a posé une question. À chaque fois que la source a servi à une réponse du chatbot, il faut que l'expert soit notifié ou son remplaçant si l'expert n'est plus dans l'entreprise. Il faudrait tout un principe de pouvoir désactiver des employés et de rediriger l'expertise à quelqu'un d'autre. Mais c'est sûr qu'il faudrait qu'il soit notifié à chaque fois que sa réponse et qu'il y ait des statistiques. Mais aussi, il faut qu'il soit notifié pour qu'il puisse facilement dire si la réponse est toujours bonne ou si elle mérite d'être mise à jour. Et si elle mérite d'être mise à jour, il faut qu'il puisse répondre oui ou non très facilement. Il faut qu'il soit notifié, il va sur la plateforme, il voit que ça a servi, il peut répondre oui ou non, est-ce que les réponses sont toujours bonnes. À ces questions-là aussi, il veut compléter la réponse ou la transformer drastiquement. Et s'il la transforme ou la modifie, il faut re-notifier l'utilisateur qui avait reçu ce conseil d'expertise dans son chatbot. Si finalement, suite à avoir notifié l'expert sur le fait que sa réponse a servi à quelqu'un, il décide de la modifier. Si l'expert a décidé, en la modifiant, d'alerter les personnes qui ont récemment reçu des réponses, il pourrait choisir de notifier les personnes pour dire que la réponse a changé. Les utilisateurs qui ont reçu cette réponse récemment recevraient une notification pour dire qu'il y a eu un changement ou une modification. Ça donnerait un résumé de la modification, soit de la subtilité qui a été changée ou carrément du changement qui a été fait. L'expert, quand il fournit une réponse, il pourrait de façon optionnelle mettre un commentaire de la durée de vie estimée de la réponse à cette question. La durée de vie va lui permettre d'avoir une date précise. Que ce soit facile en termes de UXY, de dire 6 mois, 1 an, 1 an et demi, ou en tout cas d'avoir des périodes comme ça fixe. 1 mois, 3 mois, 6 mois, 1 an, 1 an et demi, 2 ans, 3 ans, 5 ans, et pour toujours. Que ce soit facile pour lui d'essayer. Mais ça c'est vraiment optionnel, seulement dans les options avancées de la réponse à ta question. Il le spécifie comme ça, ou soit il dit en natural language la durée de vie et il explique la durée de vie selon lui et pourquoi. L'intelligence artificielle peut s'analyser un peu son commentaire pour décider d'une date de révision ou au moment où il fournit cette réponse. Quand le chatbot va aller voir dans nos let's play cette réponse-là, il va le voir le commentaire aussi. Il va pouvoir donner des précisions à l'utilisateur qui va recevoir cette réponse-là pour dire que cette réponse-là a été donnée à telle date. On savait qu'il fallait faire attention parce que c'était un produit qui évoluait vite et c'était possible que ça change beaucoup. Comme ça, ça alerte la personne qui reçoit la réponse de l'expertise que peut-être c'est à jour, peut-être ça ne l'est plus, mais du moins d'avoir ça en conséquence. C'est sûr que quand les experts vont donner des réponses aux questions et leur expertise, on va sauvegarder la date de création, les dates de modification. On va vouloir garder l'historique des réponses, des versions des réponses aussi. Aussi bien l'historique de comment la question est posée et comment la réponse est fournie. On va vouloir que l'expert, quand il donne sa réponse, que ça soit traité par l'intelligence artificielle et réécrit. En quelque sorte, réécrit d'une autre façon, que l'expert va pouvoir réviser. Mais on veut garder, c'est sûr, le prompt initial de la réponse et on veut garder la question révisée par l'intelligence artificielle. Les deux vont être dans le rack, ça va aider. En gros, toutes les questions-réponses qui vont être sauvegardées dans le Retrieval of Mental Generation, il va y avoir la question interprétée par l'intelligence artificielle, mais la question aussi posée par l'utilisateur et la façon dont lui l'a écrite. Donc, tu as la question de comment lui l'a écrite, l'intelligence qui l'a résumée en une question mieux décrite. Mais ça, les deux vont être sauvegardés et la même chose pour la réponse, on va avoir un peu le prompt de la réponse de l'expert et la réponse qui a été compliée par l'intelligence artificielle pour qu'elle soit bien propre. Et il va y avoir la date de création, les dates de modification, l'historique de toutes les versions aussi. Ça va tout être des JSON files qui vont être enregistrés dans la base de données vectorielle. Ça va être un seul JSON qui va être édité comme ça. Pour chaque question-réponse, je pense. Et les chatbots vont être connectés à cette Knowledge Base. Donc, quand il va être connecté au Knowledge, le chatbot va vraiment se fier aux réponses les plus à jour. C'est avec ça qu'il doit se fier, mais il peut quand même regarder en faisant très attention sur les réponses les plus à jour ou la formulation de la version des questions d'avant ou des réponses d'avant. Et il va devoir effectivement mettre toutes les dates, les dates de mise à jour, la durée de vie estimée, le commentaire de durée de vie estimée. Il va y avoir les tags, comme on parlait, les gens de grandes catégories qui vont permettre de filtrer, en quelque sorte, les réponses potentielles. Quand l'utilisateur parle de quoi? S'il parle d'un client spécifique, il va aller dans le tag de ce client-là pour voir les questions réponsées à ce client-là en priorité. Et s'il n'y a pas les réponses à aller voir, il y a-t-il d'autres choses? Est-ce qu'il y a d'autres clients qui avaient cette question-là avec les réponses potentielles en donnant les avertissements? Il faut que ces catégories-là, dans le logiciel, soient personnalisables. C'est un peu comme des familles de tags, mais avec des tags à la fin. La famille pourrait être les clients, et chaque tag serait le nom d'un client. La famille de tags pourrait être les départements, et chaque tag serait la maintenance, l'achat, peu importe. Il faut que toutes les questions-réponses soient toujours taguées le mieux possible. Il faut essayer d'automatiser un peu ces tags-là, et les faire confirmer par les experts. Les experts sont notifiés quand il y a une question qui demande une réponse, mais ils peuvent aussi décider d'aller remplir manuellement le knowledge base. À tout moment, ils peuvent aller dans l'application, faire le petit plus, facilement, poser la question, s'enregistrer. C'est juste « first shot » de s'enregistrer et de dire qu'on nous a posé cette question-là. Quand je dis « s'enregistrer », c'est soit d'écrire avec un texte, ou de cliquer sur le micro et de dire « je me suis fait poser la question, la semaine passée c'était l'affaire, j'ai fait des recherches, et notre réponse devrait être ci, ça, ça, ta-ta-ti, ta-ta-ta ». Ils donnent ça, et ça, ça l'envoie au Generative AI, qui va faire la question-réponse, qui va garder les textes de prompt, qui va faire les dates, qui va tout compiler ça, et qui va l'envoyer à un endpoint pour aller l'ajouter au knowledge base. Donc, aller le vectoriser, puis tout de la quitte. Fait que tous les appels avec le Generative AI, on peut les faire directement depuis l'application. Toutes les applications, tous les appels au Generated AI, on peut les faire depuis l'application. Tout sauf celle du ChatBot. Celle du ChatBot, ça va être un nouveau endpoint. Chat, ou je ne sais pas comment on va l'appeler, ChatBot. Puis lui, il va 
Donc c'est ça, celle du ChatBot. Elle, ça va être un endpoint à part entière qui va s'appeler ChatBot, qui va pouvoir envoyer le Session ID à l'intérieur ou le Conversation ID, peu importe comment qu'on l'appelle, puis peut-être les tags aussi. Fait que l'utilisateur, quand il démarre un chat, il peut démarrer un chat général ou il peut l'associer à des tags pour aider le ChatBot à bien filtrer l'information. Ça va envoyer la question avec le Session ID, fait qu'en gros, le prompt avec le Session ID de la conversation, puis les tags s'ils sont spécifiés, s'ils sont optionnels. Et après ça, nous, dans la NullTerm, on va appeler le bon agent qui va aller retrouver l'information dans le Knowledge Base, dans le Retrieval Augmented Generation. Mais toutes ces conversations-là, dans la NullTerm avec le Session ID, il ne va pas les retrouver pour poursuivre les conversations. Mais moi, je veux quand même les sauvegarder dans la base de données. Ça va être avec ces endpoints normaux. Dans l'application WebBase, il va appeler ces endpoints à chaque fois qu'il y a un nouveau message, à chaque fois qu'il y a une nouvelle réponse. Il va aller sauvegarder ça dans la base de données avec ces endpoints en lien avec ça. Comme ça, il va pouvoir conserver toutes les Session ID qui correspondent aux conversations, démarrer des nouvelles conversations, puis voir tout l'historique des messages. S'il décide d'envoyer un nouveau prompt, ça envoie seulement le Session ID, pas besoin d'envoyer l'ensemble de l'historique des messages. Parce que dans N8Term, on va aller trouver la même base de données avec le Session ID. Donc, l'utilisateur peut démarrer des nouvelles conversations, il voit son historique. Là, en ce moment, on va faire que tous les utilisateurs, quand ils se connectent, ils voient tous les historiques de tout le monde. Ils peuvent aller aussi éditer l'expertise de tous les experts. Donc, d'aller dans les experts, créer des nouveaux experts, éditer les textes d'expertise. Peut-être qu'on peut associer aussi des tags à certains experts. Ça, ça peut être pertinent. Comme je disais, tu vas utiliser tous tes endpoints habituels qu'il y a dans ton instruction système. C'est-à-dire, les endpoints pour interagir avec la base de données, les endpoints pour interagir avec la transcription, parce que tous les champs de texte doivent avoir le petit micro. Quand je clique dessus, je peux parler. Quand je reclique, ça vient transcrire et coller le texte dans la boîte de texte. Ça va être les endpoints standards. Tous les appels au generative AI, ça va être les endpoints standards, sauf celles du chatbot. Quand l'utilisateur va poser une question à son chatbot, et qu'il n'y aura pas eu de réponse, on va vouloir créer ce nouvel objet-là, JSON, dans la base de données, comme étant une question qui mérite une réponse de la part d'un expert. On va vouloir la créer en gardant le prompt, en gardant la question en natural language que l'utilisateur a posée. On va vouloir aussi la travailler avec le generative AI pour la résumer ou la poser différemment, l'éclaircir. Tous les appels au generative AI vont se faire par les endpoints ASK qu'il y a dans les instructions API. À chaque fois qu'on appelle ASK, on va bien spécifier le JSON de sortie qu'on s'attend à output. C'est comme si ça créait tranquillement le JSON d'une question qui mérite une réponse, ça va notifier les experts. Ça va devoir faire une analyse de cette question-là, c'est quel expert. Cette analyse-là, ça va être le generative AI, encore une fois qu'il va le faire, qui va prendre la question, aller lire tous les textes de tous les experts, puis peut-être, en fonction de la catégorie aussi, trouver plus facilement quel expert pourrait être la bonne personne. Mais s'il n'y a pas de tags, s'il n'y a pas de catégories, quand je dis catégories, c'est des tags, les familles de tags, en quelque sorte. On doit pouvoir, dans le système, dans le panneau d'administration, oui, créer des experts, mais aussi leur expertise, pouvoir leur associer des tags. Il faut pouvoir créer les familles de tags. Il y a une famille qui peut être, disons, une famille de clients. Chaque tag, ça va être les clients, une famille de départements. Chaque tag, dans cette famille de tags, va être les différents départements de cette entreprise-là. Les experts peuvent être taggés, les questions-réponses peuvent être taggées aussi. Il peut y avoir plusieurs tags. On va appeler le chatbot. C'est sûr, on va pouvoir l'appeler aussi par tags pour faciliter la recherche. Mais c'est optionnel. Les tags sont toujours optionnels. Là, c'est ça. L'utilisateur, lui, pose sa question au chatbot. Le chatbot va donner sa réponse dans le Knowledge Base. Mais s'il n'y a pas la réponse, il va activer un peu le mode de dire « j'ai pas trouvé la réponse, mais j'ai notifié de l'expert pour qu'il puisse nous fournir l'information. » Et de mentionner que quand l'expert va avoir donné sa réponse, on va être re-notifié. L'utilisateur qui a posé la question va être re-notifié pour dire qu'il a une réponse à sa question. Tous les utilisateurs, pour l'instant, ont accès à toutes les configurations. C'est juste que quand tu te connectes comme un utilisateur, peut-être dans l'URL en haut, l'URL est différente selon l'utilisateur. On gérera la gestion des utilisateurs avec les droits d'accès. Pour l'instant, on donne accès à tout le monde, mais tu peux te connecter comme un utilisateur selon ton URL en haut. Là, tu vois toutes les questions qui attendent ton expertise, qui attendent tes réponses. Tu peux les réviser une à une et fournir une réponse. Quand tu fournis une réponse, tu la fournis en langage naturel. Il faut que ce soit très simple. Tu es un expert, tu reçois une notification courriel, un SMS, un lien direct. Dans ton lien direct, ça se connecte en étant toi, et ça se connecte à la bonne page où tu peux réviser les questions que les gens attendent tes réponses. Là, c'est une vue très mobile. Toute l'application a été développée surtout pour être fonctionnelle mobile. Tu peux répondre facilement avec un champ de texte ou le micro. Toutes ces données vont être sauvegardées dans le même JSON. En gardant en mémoire la question initiale de l'utilisateur en langage naturel, la question interprétée par l'IA. L'expert va voir la question interprétée par l'IA. Il peut voir de façon optionnelle la question initiale. Il va fournir sa réponse. Ça va updater le JSON avec la réponse en langage naturel, la réponse interprétée par l'IA. Il va pouvoir la catégoriser manuellement ou se faire proposer des catégories. Soit réviser, peut-être qu'elle va avoir déjà été précatégorisée selon l'utilisateur. Si l'utilisateur veut démarrer une conversation avec des catégories de texte, c'est sûr que la question pourrait être précatégorisée. L'expert va pouvoir la catégoriser ou pas de façon optionnelle. Il va pouvoir mettre son commentaire de durée de vie. Tout ça, c'est optionnel. Ce qui est important, c'est que pour lui, ça soit très simple de fournir la réponse. Tout le reste, c'est optionnel. Il peut juste dérouler les options s'il le souhaite. Peut-être même que dans la réponse qu'il a donnée en langage naturel, il va peut-être avoir déjà des réponses aux champs optionnels. Il va peut-être avoir dit, ça c'est dans le texte, telle affaire, la durée de vie d'ailleurs est telle-telle. Il va falloir l'interpréter, ce texte-là, de l'utilisateur, pour pouvoir aller remplir les champs automatiques, si on veut, optionnels. Là, ça va notifier la personne ou les personnes qui avaient posé ces questions-là récemment pour dire qu'une nouvelle réponse a été fournie. Même chose, ça va envoyer des notifications à la personne qui va pouvoir se connecter avec un lien direct pour voir les nouvelles réponses, etc. Les endpoints, à chaque fois qu'il y a du AI, qu'il y a de l'interprétation comme ça, c'est les endpoints d'habituels ASK. On va utiliser Gemini 2.5 Flash comme modèle principal. C'est ça, la seule route que je disais qui est particulière. Toutes les routes vont être les routes standard de transcription, de data, les liens avec la database, les liens avec le AI. Ça, ça va tout être classique. Par contre, les routes un peu custom pour cet outil-là, il va y avoir la route chatbot que j'ai mentionnée. Il va y avoir la route pour fider le chatbot. En gros, à chaque fois qu'il y a eu une nouvelle réponse d'un expert, la donnée va avoir été updatée dans la base de données de la même façon qu'à l'habitude. Vu ça, je veux vraiment garder tous les JSON avec toutes les versionnings, etc. Mais ça va y avoir une nouvelle façon de fider le chatbot. Ça va y avoir été envoyé aussi au...
L'interface mobile est très responsable, avec des menus dans le bas qui sont sticky et facile à naviguer. Chaque utilisateur peut aller éditer les experts pour l'instant. Tout le temps la gestion des experts, on va refaire ça plus tard. Mais c'est juste quand tu te connais comme un expert dans l'URL, on voit quel expert. Puis là, tu as tes pages à toi, comme expert. Tu peux aller dans le panneau d'admin et aller éditer la liste des experts, leur texte d'expertise, des choses comme ça. On va leur mettre des tags aussi à ces experts-là. Mais tu peux aussi voir tes notifications. Donc, tes notifications de ton expertise qui est attendue, des questions qui attendent tes réponses. Tu leur fournis tes réponses facilement. Tu as tes pages de notifications comme ça. Tu as tes pages de notifications aussi. Je ne sais pas si c'est dans la même, on verra. Mais à chaque fois que tes réponses ont été utilisées, tu seras notifié pour pouvoir dire rapidement, est-ce que tu es toujours à jour ou pas à jour de cette réponse-là. Et si elle n'est pas à jour, tu pourras t'enregistrer pour dire pourquoi elle n'est pas à jour. Puis ça va créer une nouvelle version. Puis ça va aller re-notifier l'utilisateur qui avait posé cette question-là. C'est-à-dire qu'il y a une nouvelle réponse qui a été mise. Ça va updater la donnée dans le JSON de la base de données MongoDB. Chaque question-réponse, c'est un JSON qui est updaté au fur et à mesure, qui est versionné, qui a les tags d'associés, les catégories de tags, qui a les questions-réponses en natural language interprétées par l'AI, qui a les versionnings, tout ça. Puis à chaque fois qu'un expert vient l'updater, que ce soit un update sur une révision qui dit qu'elle mérite une révision ou soit qu'il est allé la sélectionner manuellement dans la liste des questions-réponses. Lui, il a une page où il peut voir toutes les questions-réponses qui existent. Il peut naviguer à l'intérieur par des mots-clés ou les filtrer par tag. Il peut aller les éditer, soit manuellement, soit avec l'AI en enregistrant un texte. Ça va créer des nouvelles versions. Puis à chaque fois, ça va aller updater le JSON de ce Q&A-là dans MongoDB. Mais aussi, ça va appeler la nouvelle route RAG, on va l'appeler, mais qui feed la base de données vectorielle avec le nouveau JSON qui a été updaté ou qui a été créé. Le utilisateur, en termes d'expérience, le utilisateur, pour que ce soit très simple, il a accès à ces chatbots très rapidement. C'est là qu'il peut aller naviguer très rapidement. Ou il peut aller, en tant qu'expert, à ces zones de notification très vite pour voir les réponses qui sont attendues dessus de sa part. Démarrer le micro ou le champ de texte. Toutes les réponses qui sont optionnelles sur la date d'expiration de cette réponse-là, les tags, tout ça, il n'y a pas à les remplir. Ils ne prennent pas de l'espace mental. Ils sont juste optionnels. Il peut les dérouler s'il veut les faire. Mais si dans le texte de la réponse qu'il a mis, il y en a parlé, il faut faire attention parce que cette réponse sera plus bonne plus tard, peut-être dans six mois. D'ailleurs, si c'est en lien seulement avec tel client ou tel truc, ça va venir aussi faire des asks AI. Ça va envoyer au AI pour l'interpréter et ça va peut-être remplir les champs automatiques de catégories, de tags, mais aussi de dates d'expiration, de textes d'expiration et tout ça. Il peut juste les confirmer s'il veut. C'est ça. L'utilisateur se connecte. Il peut avoir ses chats. Il navigue dans une sidebar historique de ses conversations. Il peut poursuivre une conversation. Dans le bas, il y a des menus comme un peu iOS avec des icônes. Il peut aller voir les notifications qu'il a, qu'il lui demande une réponse de son expertise ou à chaque fois que son expertise a été utilisée pour pouvoir réviser si les réponses sont toujours bonnes. Il peut aussi voir les notifications quand lui attendait les réponses d'un expert. C'est-à-dire que s'il y a un expert qui a changé une réponse à une question que lui a posée dans un chatbot récemment, ça va le notifier s'il y a une nouvelle réponse. Il y a le panneau de configuration aussi. Je veux que ça soit en light mode a priori, mais dans le panneau de configuration, on peut aussi le configurer en dark mode. Ça sépare l'utilisateur évidemment. C'est une application vraiment intuitive, simple. Moi, j'aime beaucoup le Liquid Glass, d'ailleurs, donc n'hésite pas à l'utiliser. Je veux que ça soit vraiment intuitif. Notre couleur d'entreprise primaire, c'est le 0078FF. Donc, ça serait bien de le mettre. On décidera plus tard si cette couleur primaire, à cette légère-là, on la fait varier. Il faut que ça fonctionne très bien mobile, très simple. Ah oui, je veux aussi avoir un dashboard, donc un onglet dashboard, où tu vois plein de statistiques sur le nombre d'experts, le nombre de Q&A qui existe, le nombre de fois qu'ils ont été utilisés dans la période donnée. Donc, on va changer la période de temps. Regarder le dernier mois, le dernier trimestre, le dernier six mois, la dernière année, peu importe. Ou toutes les périodes. De voir aussi à chaque fois qu'il y a eu des réponses qui ont été utilisées. Et toi, en tant qu'expert, avoir des stats liées à ton expertise sur le nombre de fois que tu as... le nombre de Q&A qui sont en lien avec toi, le nombre de fois que ça a été utilisé, etc. Le nombre de fois que ça a été utilisé, etc. Donc, chaque Q&A, question-réponse, va falloir lui associer un expert qui est la personne qui l'a créé ou qui l'a mis à jour. Il peut y avoir plusieurs experts. Comme ça, on va pouvoir avoir les stats là-dessus. Il faut que toutes ces statistiques se sauvegardent et se présentent dans la zone dashboard pour motiver les gens. Et pouvoir naviguer. Si tu vois que ton expertise a été utilisée dix fois dans le dernier mois, de pouvoir voir c'est quel Q&A qui a été utilisé, est-ce que tu veux les réviser. À tout moment, tu peux aller filtrer tes Q&A, soit depuis le dashboard, soit dans la liste des Q&A, les filtrer par ton expertise, puis aller les éditer, les améliorer, et que ça soit très simple de le faire. Tu t'enregistres, tu démontres le micro, tu envoies du texte, et ça vient l'updater. Ça vient le mettre à jour, et quand ça le met à jour, ça le met à jour. Oui, dans MongoDB, avec les routes standard d'instruction que tu as dans ton instruction system. Mais aussi par... Ça va aller l'updater dans la base de données vectorielle en appelant la nouvelle route pour l'updater sur la base de données vectorielle. Chaque update, ça doit le faire. Ce qu'il faut savoir, c'est qu'il y a très peu de back-end dans cet outil-là, à part le chatbot et la mise à jour du rack. C'est-à-dire que, si l'utilisateur démarre un chat, puis là, dans le chat, ça l'envoie au chatbot. Le chatbot détermine s'il peut répondre ou s'il ne peut pas répondre. S'il peut répondre, il renvoie la réponse, mais il va renvoyer aussi justement le ID de la réponse, tout ça. Et s'il ne peut pas répondre, il va envoyer qu'il ne peut pas le faire. C'est au niveau front-end que ça va aller faire la recherche de l'expertise. Dans le body du JSON du chatbot, ça va dire « Ah, ok, je n'ai pas pu répondre. » Donc, il y a un champ qui va servir à ça. Ce que ça va faire pour l'utilisateur, ça va dire « Ah, il n'y a pas eu de réponse dans le knowledge base en lien avec ça. » Le chatbot va avoir répondu « Je n'ai pas pu répondre. » Donc, on va afficher ce texte-là. Mais il va avoir un champ aussi dans le JSON body du webbook response qui va dire « Ah, ça n'a pas pu répondre. » Donc, côté front-end, on va voir dans le chatbot qu'on fait une recherche de l'expert qui pourrait y répondre et le dire « Ah, tel expert a été trouvé et a été notifié en attente de son expertise. » Et que tout ça, ça se fasse front-end. Et la même chose, quand tu es un expert et que tu as reçu une notification courriel, tu reçois la notification courriel avec « Oh, on attend notre expertise. » et un résumé courriel. Donc, ça va avoir arrivé les bonnes routes HTML, les bonnes routes email, les beaux emails avec HTML, les bonnes routes aussi SMS. Puis là, tu as un lien. Ça donne un peu la question-réponse. Puis, la question est attendue. Puis là, tu parles et il y a un lien direct pour aller soit justement répondre à cette question-là ou recevoir une notification aussi à chaque fois que sa réponse a été utilisée pour voir si elle est toujours d'actualité. Puis là, il va accéder directement à cette question-réponse-là pour pouvoir l'éditer en quelque sorte. Puis quand il va l'éditer, ça va l'éditer. Puis c'est tout front-end. Il n'y a pas de back-end. Ça fait que c'est front-end que ça va appeler les bonnes routes pour l'abandonner, pour éditer son JSON, pour le RAG, pour l'éditer.
Fait que c'est ça, fait que... T'sais, ça va aller titer le JSON, ça va titer le RAG avec la nouvelle route, pis ça va notifier la personne en question. Fait que si, mettons, c'est l'expert qui a donné sa réponse, ben là, faut que ça l'aille notifier l'utilisateur qui attend cette réponse-là, t'sais. Fait que là, c'est vraiment front-end. C'est front-end, ça va dire « Oh, ok, ben là, l'utilisateur qui attendait ta réponse va être notifié, pis là, ben, ça fait les notifications, ça fait les boss appellent les PIs pour aller les notifier, pis ça avertit que ça l'a fait. Fait que ça veut dire que chaque question qui attend des réponses, chaque question qui est une nouvelle question dans la base de données, ben, faut garder en mémoire c'était qui l'utilisateur qui a posé cette question-là, pour pouvoir le notifier ensuite correctement. Fait que quand on le notifie, ben, il faut appeler les routes de notification e-mail ou SMS, mais avec le bon utilisateur pour avoir le bon courriel, le bon numéro de téléphone, etc., pour pouvoir le notifier. Fait que là, l'utilisateur va recevoir une notification par courriel lui aussi. « Ah, ta réponse, il y a une nouvelle réponse à la question, ou la réponse a été updatée, parce que c'est le même principe. Quand l'expert a reçu une notification pour dire que sa réponse a été utilisée, ben, il peut l'updater, et ça va updater l'utilisateur qui vient de recevoir cette question-là. Fait qu'il faut tout garder ça dans la base de données, d'une façon ou d'une autre. Pis, updater, ben, notifier les bonnes personnes. Fait que le seul back-end, c'est vraiment le RAG, le chatbot, pis le RAG. Sinon, tout le reste, c'est vraiment le front-end qui va décider d'aller notifier les gens, pis tout ça. Pis d'avertir, pis d'avoir un feedback d'utilisateur intuitif. Fait que voilà, ça fait le tour un peu de l'application. Fait que l'application, elle sert à quoi? Ben, elle sert à créer une base de connaissances incroyables. La problématique, c'est que ce soit en usine, ou que ce soit dans n'importe quelle entreprise, ben, il y a plein d'informations qui sont juste dans la tête des gens. Pis ces gens-là, ben, il y a du roulement de personnel, il y a des gens qui tombent en retraite, pis ils partent avec cette expertise-là. Il y a des questions qui sont sans réponse, pis c'est difficile de savoir qui a cette réponse-là. Pis là, l'intelligence artificielle générative nous permet vraiment d'aller beaucoup plus loin, pis de documenter l'ensemble des connaissances de l'entreprise. Par contre, il faut que ces connaissances-là soient mises à jour, pis s'ils ne sont pas mis à jour, c'est quasiment pire d'avoir des mauvaises connaissances qui viennent filer des mauvaises informations. Fait que toute la logique de mise à jour, c'est super pertinent aussi. Fait que ça devient très, très simple d'avoir la propriété intellectuelle de l'entreprise, le savoir-faire qui est redonné à l'entreprise, qui est mis sur papier, qui est accessible à l'ensemble des parties prenantes de cette entreprise-là, qui est bien catégorisée par des tags, et donc, selon tes accès, tu as accès à des données plus ou moins sensibles, selon si tu as accès seulement à certains tags ou autres. Et c'est ça, tu peux naviguer comme ça dans la connaissance, pis surtout, c'est très difficile à maintenir, à créer cette connaissance-là, mais en ayant un système comme ça d'apprentissage automatique, ça devient très simple, ça découpe la tâche en plein de petites tâches, pis ça, c'est très facile pour n'importe quel gestionnaire. En fin de journée, il y a eu trois questions aujourd'hui, je réponds aux trois questions, bim, bam, boum, je sais que ça va alerter les bonnes personnes, je sais que ça va conserver ça dans l'historique, dans notre documentation, que ça va pouvoir être réutilisé plus tard, c'est ce que tout le monde rêve, en fait. C'est un outil vraiment incroyable qui bénéficie de l'intelligence artificielle générative, tout en ne minimisant pas la sécurité informatique, la sécurité de la donnée, ça utilise des modèles où les politiques sont propres aux besoins de l'entreprise, et donc, c'est ça.

@Martin Landry, comme discuté un outil interne pour répondre aux question des employés tout en créant une kwonledge de l'entreprise. cc @Arthur Le Saint

Voici une description de l'outil qui peut servir aussi de prompt de conception.

OK, donc voici la description d'un d'un produit d'un outil interne qu'on pourrait aussi potentiellement commercialiser l'idée, c'est de pouvoir poser dans enfin. En fait on va partir plutôt de la problématique. La problématique c'est que dans l'entreprise souvent des employés ont différentes questions sur par exemple ça peut-être sur des nouveaux produits qui viennent d'être développés. Donc les équipes de cess ou les équipes de marketing ou les équipes de vente se disent OK. Est-ce que c'est techniquement possible pour le client de faire ça dans dans le nouveau produit qu'on a développé Puis actuellement la question est posée en direct d'équipe à équipe, mais on n'a aucune traçabilité, on n'enregistre pas la question, on fait pas de documentation ce qui fait que ça peut arriver très souvent que la même personne le même responsable du département de l'entreprise se fasse poser plusieurs fois la même question donc l'idée ce serait de faire un outil qui permet de capter et de capturer une question et de fournir une réponse. Donc on pourrait je sais pas encore quel nom on va donner à ça mais pour l'instant appelons-le. Question un expert. Donc ça veut dire qu'on a un employé imaginons lelow, on a un employé. Qui a une question, il la pose dans l'outil, il pose la question dans l'outil S réponse qui a déjà été enregistrée par un expert donc un expert c'est un employé de l'entreprise qui maîtrise le sujet. Donc si l'employé pose la question dans l'outil et que il existe une réponse parce que cette question a déjà été posée auparavant et a été répandue par un expert dans l'outil. Et bien l'outil donne la réponse à la question posée par l'employé si l'employé pose une question qui n'existe pas et bien ça va être segmenté, je pense par département d'entreprise en fonction de la question posée donc ça peut-être soit fait automatiquement ou soit avec un sécteur. Donc l'employé pose une question, elle n'existe pas, il y a pas de réponse donc ça va notifier l'expert concerné dans le bon département de l'entreprise en lui disant hey salut y a y a Jean-Jacques qui a une question sur tel sujet auquel l'expert enregistre sa réponse en fonction du type de questions en fonction de la réponse, l'expert peut également sélectionner une durée de vie d'une réponse approximative entre est-ce que ça va être une réponse qui va vite évoluer ? Est-ce que c'est une réponse moyen terme ou est-ce que c'est une réponse fixe long terme donc par exemple comment s'appelle l'entreprise. Bah intelligence industrielle ça c'est plutôt une réponse qui va pas évoluer sur le long terme Bah c'est ce sera toujours la même réponse sur le long terme, alors que par exemple quel a été notre chiffre d'affaire de la semaine dernière bon bah là la réponse est très courte et très vite évolutive Ça ça va permettre plus tard, l'objectif donc ça c'est un second objectif au-delà de la documentation des questions c'est que cette documentation là reste à jour. Donc pour ça ce qui va se passer, c'est que imaginons qu'un expert a dit qu'une réponse était viable sur le moyen terme, maisons six mois. Eh bien au bout de six mois si un employé repose la même question, le et que la réponse existe et qu'elle a été notifiée pour une durée de vie de six mois Bah le l'expert sera notifiée et l'outil lui demandera si cette réponse est toujours valide ou non et du coup on pourra soit dire oui avec deux petits boutons donc un bouton pour répondre oui donc la réponse est toujours actuelle et toujours actuellement valide ou un petit bouton enregistrer une nouvelle réponses à la question

